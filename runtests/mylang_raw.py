# The file was automatically generated by Lark v0.11.3
__version__ = "0.11.3"

#
#
#   Lark Stand-alone Generator Tool
# ----------------------------------
# Generates a stand-alone LALR(1) parser with a standard lexer
#
# Git:    https://github.com/erezsh/lark
# Author: Erez Shinan (erezshin@gmail.com)
#
#
#    >>> LICENSE
#
#    This tool and its generated code use a separate license from Lark,
#    and are subject to the terms of the Mozilla Public License, v. 2.0.
#    If a copy of the MPL was not distributed with this
#    file, You can obtain one at https://mozilla.org/MPL/2.0/.
#
#    If you wish to purchase a commercial license for this tool and its
#    generated code, you may contact me via email or otherwise.
#
#    If MPL2 is incompatible with your free or open-source project,
#    contact me and we'll work it out.
#
#

from io import open



class LarkError(Exception):
    pass


class ConfigurationError(LarkError, ValueError):
    pass


def assert_config(value, options, msg='Got %r, expected one of %s'):
    if value not in options:
        raise ConfigurationError(msg % (value, options))


class GrammarError(LarkError):
    pass


class ParseError(LarkError):
    pass


class LexError(LarkError):
    pass


class UnexpectedInput(LarkError):
    #--
    pos_in_stream = None
    _terminals_by_name = None

    def get_context(self, text, span=40):
        #--
        assert self.pos_in_stream is not None, self
        pos = self.pos_in_stream
        start = max(pos - span, 0)
        end = pos + span
        if not isinstance(text, bytes):
            before = text[start:pos].rsplit('\n', 1)[-1]
            after = text[pos:end].split('\n', 1)[0]
            return before + after + '\n' + ' ' * len(before.expandtabs()) + '^\n'
        else:
            before = text[start:pos].rsplit(b'\n', 1)[-1]
            after = text[pos:end].split(b'\n', 1)[0]
            return (before + after + b'\n' + b' ' * len(before.expandtabs()) + b'^\n').decode("ascii", "backslashreplace")

    def match_examples(self, parse_fn, examples, token_type_match_fallback=False, use_accepts=False):
        #--
        assert self.state is not None, "Not supported for this exception"

        if isinstance(examples, dict):
            examples = examples.items()

        candidate = (None, False)
        for i, (label, example) in enumerate(examples):
            assert not isinstance(example, STRING_TYPE)

            for j, malformed in enumerate(example):
                try:
                    parse_fn(malformed)
                except UnexpectedInput as ut:
                    if ut.state == self.state:
                        if use_accepts and hasattr(self, 'accepts') and ut.accepts != self.accepts:
                            logger.debug("Different accepts with same state[%d]: %s != %s at example [%s][%s]" %
                                         (self.state, self.accepts, ut.accepts, i, j))
                            continue
                        try:
                            if ut.token == self.token:  ##

                                logger.debug("Exact Match at example [%s][%s]" % (i, j))
                                return label

                            if token_type_match_fallback:
                                ##

                                if (ut.token.type == self.token.type) and not candidate[-1]:
                                    logger.debug("Token Type Fallback at example [%s][%s]" % (i, j))
                                    candidate = label, True

                        except AttributeError:
                            pass
                        if candidate[0] is None:
                            logger.debug("Same State match at example [%s][%s]" % (i, j))
                            candidate = label, False

        return candidate[0]

    def _format_expected(self, expected):
        if self._terminals_by_name:
            d = self._terminals_by_name
            expected = [d[t_name].user_repr() if t_name in d else t_name for t_name in expected]
        return "Expected one of: \n\t* %s\n" % '\n\t* '.join(expected)


class UnexpectedEOF(ParseError, UnexpectedInput):
    def __init__(self, expected, state=None, terminals_by_name=None):
        self.expected = expected
        self.state = state
        from .lexer import Token
        self.token = Token("<EOF>", "")  ##

        self.pos_in_stream = -1
        self.line = -1
        self.column = -1
        self._terminals_by_name = terminals_by_name

        super(UnexpectedEOF, self).__init__()

    def __str__(self):
        message = "Unexpected end-of-input. "
        message += self._format_expected(self.expected)
        return message


class UnexpectedCharacters(LexError, UnexpectedInput):
    def __init__(self, seq, lex_pos, line, column, allowed=None, considered_tokens=None, state=None, token_history=None,
                 terminals_by_name=None, considered_rules=None):
        ##

        self.line = line
        self.column = column
        self.pos_in_stream = lex_pos
        self.state = state
        self._terminals_by_name = terminals_by_name

        self.allowed = allowed
        self.considered_tokens = considered_tokens
        self.considered_rules = considered_rules
        self.token_history = token_history

        if isinstance(seq, bytes):
            self.char = seq[lex_pos:lex_pos + 1].decode("ascii", "backslashreplace")
        else:
            self.char = seq[lex_pos]
        self._context = self.get_context(seq)

        super(UnexpectedCharacters, self).__init__()

    def __str__(self):
        message = "No terminal matches '%s' in the current parser context, at line %d col %d" % (self.char, self.line, self.column)
        message += '\n\n' + self._context
        if self.allowed:
            message += self._format_expected(self.allowed)
        if self.token_history:
            message += '\nPrevious tokens: %s\n' % ', '.join(repr(t) for t in self.token_history)
        return message


class UnexpectedToken(ParseError, UnexpectedInput):
    #--

    def __init__(self, token, expected, considered_rules=None, state=None, interactive_parser=None, terminals_by_name=None, token_history=None):
        ##

        self.line = getattr(token, 'line', '?')
        self.column = getattr(token, 'column', '?')
        self.pos_in_stream = getattr(token, 'pos_in_stream', None)
        self.state = state

        self.token = token
        self.expected = expected  ##

        self._accepts = NO_VALUE
        self.considered_rules = considered_rules
        self.interactive_parser = interactive_parser
        self._terminals_by_name = terminals_by_name
        self.token_history = token_history

        super(UnexpectedToken, self).__init__()

    @property
    def accepts(self):
        if self._accepts is NO_VALUE:
            self._accepts = self.interactive_parser and self.interactive_parser.accepts()
        return self._accepts

    def __str__(self):
        message = ("Unexpected token %r at line %s, column %s.\n%s"
                   % (self.token, self.line, self.column, self._format_expected(self.accepts or self.expected)))
        if self.token_history:
            message += "Previous tokens: %r\n" % self.token_history

        return message

    @property
    def puppet(self):
        warn("UnexpectedToken.puppet attribute has been renamed to interactive_parser", DeprecationWarning)
        return self.interactive_parser
    


class VisitError(LarkError):
    #--

    def __init__(self, rule, obj, orig_exc):
        self.obj = obj
        self.orig_exc = orig_exc

        message = 'Error trying to process rule "%s":\n\n%s' % (rule, orig_exc)
        super(VisitError, self).__init__(message)


import sys, re
import logging
from io import open
logger = logging.getLogger("lark")
logger.addHandler(logging.StreamHandler())
##

##

logger.setLevel(logging.CRITICAL)

if sys.version_info[0]>2:
    from abc import ABC, abstractmethod
else:
    from abc import ABCMeta, abstractmethod
    class ABC(object): ##

        __slots__ = ()
        __metclass__ = ABCMeta


Py36 = (sys.version_info[:2] >= (3, 6))

NO_VALUE = object()


def classify(seq, key=None, value=None):
    d = {}
    for item in seq:
        k = key(item) if (key is not None) else item
        v = value(item) if (value is not None) else item
        if k in d:
            d[k].append(v)
        else:
            d[k] = [v]
    return d


def _deserialize(data, namespace, memo):
    if isinstance(data, dict):
        if '__type__' in data:  ##

            class_ = namespace[data['__type__']]
            return class_.deserialize(data, memo)
        elif '@' in data:
            return memo[data['@']]
        return {key:_deserialize(value, namespace, memo) for key, value in data.items()}
    elif isinstance(data, list):
        return [_deserialize(value, namespace, memo) for value in data]
    return data


class Serialize(object):
    #--

    def memo_serialize(self, types_to_memoize):
        memo = SerializeMemoizer(types_to_memoize)
        return self.serialize(memo), memo.serialize()

    def serialize(self, memo=None):
        if memo and memo.in_types(self):
            return {'@': memo.memoized.get(self)}

        fields = getattr(self, '__serialize_fields__')
        res = {f: _serialize(getattr(self, f), memo) for f in fields}
        res['__type__'] = type(self).__name__
        postprocess = getattr(self, '_serialize', None)
        if postprocess:
            postprocess(res, memo)
        return res

    @classmethod
    def deserialize(cls, data, memo):
        namespace = getattr(cls, '__serialize_namespace__', {})
        namespace = {c.__name__:c for c in namespace}

        fields = getattr(cls, '__serialize_fields__')

        if '@' in data:
            return memo[data['@']]

        inst = cls.__new__(cls)
        for f in fields:
            try:
                setattr(inst, f, _deserialize(data[f], namespace, memo))
            except KeyError as e:
                raise KeyError("Cannot find key for class", cls, e)
        postprocess = getattr(inst, '_deserialize', None)
        if postprocess:
            postprocess()
        return inst


class SerializeMemoizer(Serialize):
    #--

    __serialize_fields__ = 'memoized',

    def __init__(self, types_to_memoize):
        self.types_to_memoize = tuple(types_to_memoize)
        self.memoized = Enumerator()

    def in_types(self, value):
        return isinstance(value, self.types_to_memoize)

    def serialize(self):
        return _serialize(self.memoized.reversed(), None)

    @classmethod
    def deserialize(cls, data, namespace, memo):
        return _deserialize(data, namespace, memo)


try:
    STRING_TYPE = basestring
except NameError:   ##

    STRING_TYPE = str


import types
from functools import wraps, partial
from contextlib import contextmanager

Str = type(u'')
try:
    classtype = types.ClassType  ##

except AttributeError:
    classtype = type    ##



def smart_decorator(f, create_decorator):
    if isinstance(f, types.FunctionType):
        return wraps(f)(create_decorator(f, True))

    elif isinstance(f, (classtype, type, types.BuiltinFunctionType)):
        return wraps(f)(create_decorator(f, False))

    elif isinstance(f, types.MethodType):
        return wraps(f)(create_decorator(f.__func__, True))

    elif isinstance(f, partial):
        ##

        return wraps(f.func)(create_decorator(lambda *args, **kw: f(*args[1:], **kw), True))

    else:
        return create_decorator(f.__func__.__call__, True)


try:
    import regex
except ImportError:
    regex = None

import sre_parse
import sre_constants
categ_pattern = re.compile(r'\\p{[A-Za-z_]+}')

def get_regexp_width(expr):
    if regex:
        ##

        ##

        ##

        regexp_final = re.sub(categ_pattern, 'A', expr)
    else:
        if re.search(categ_pattern, expr):
            raise ImportError('`regex` module must be installed in order to use Unicode categories.', expr)
        regexp_final = expr
    try:
        return [int(x) for x in sre_parse.parse(regexp_final).getwidth()]
    except sre_constants.error:
        raise ValueError(expr)


from collections import OrderedDict


class Meta:
    def __init__(self):
        self.empty = True


class Tree(object):
    #--
    def __init__(self, data, children, meta=None):
        self.data = data
        self.children = children
        self._meta = meta

    @property
    def meta(self):
        if self._meta is None:
            self._meta = Meta()
        return self._meta

    def __repr__(self):
        return 'Tree(%r, %r)' % (self.data, self.children)

    def _pretty_label(self):
        return self.data

    def _pretty(self, level, indent_str):
        if len(self.children) == 1 and not isinstance(self.children[0], Tree):
            return [indent_str*level, self._pretty_label(), '\t', '%s' % (self.children[0],), '\n']

        l = [indent_str*level, self._pretty_label(), '\n']
        for n in self.children:
            if isinstance(n, Tree):
                l += n._pretty(level+1, indent_str)
            else:
                l += [indent_str*(level+1), '%s' % (n,), '\n']

        return l

    def pretty(self, indent_str='  '):
        #--
        return ''.join(self._pretty(0, indent_str))

    def __eq__(self, other):
        try:
            return self.data == other.data and self.children == other.children
        except AttributeError:
            return False

    def __ne__(self, other):
        return not (self == other)

    def __hash__(self):
        return hash((self.data, tuple(self.children)))

    def iter_subtrees(self):
        #--
        queue = [self]
        subtrees = OrderedDict()
        for subtree in queue:
            subtrees[id(subtree)] = subtree
            queue += [c for c in reversed(subtree.children)
                      if isinstance(c, Tree) and id(c) not in subtrees]

        del queue
        return reversed(list(subtrees.values()))

    def find_pred(self, pred):
        #--
        return filter(pred, self.iter_subtrees())

    def find_data(self, data):
        #--
        return self.find_pred(lambda t: t.data == data)


from inspect import getmembers, getmro


class Discard(Exception):
    #--
    pass

##



class _Decoratable:
    #--

    @classmethod
    def _apply_decorator(cls, decorator, **kwargs):
        mro = getmro(cls)
        assert mro[0] is cls
        libmembers = {name for _cls in mro[1:] for name, _ in getmembers(_cls)}
        for name, value in getmembers(cls):

            ##

            if name.startswith('_') or (name in libmembers and name not in cls.__dict__):
                continue
            if not callable(value):
                continue

            ##

            if hasattr(cls.__dict__[name], 'vargs_applied') or hasattr(value, 'vargs_applied'):
                continue

            static = isinstance(cls.__dict__[name], (staticmethod, classmethod))
            setattr(cls, name, decorator(value, static=static, **kwargs))
        return cls

    def __class_getitem__(cls, _):
        return cls


class Transformer(_Decoratable):
    #--
    __visit_tokens__ = True   ##


    def __init__(self,  visit_tokens=True):
        self.__visit_tokens__ = visit_tokens

    def _call_userfunc(self, tree, new_children=None):
        ##

        children = new_children if new_children is not None else tree.children
        try:
            f = getattr(self, tree.data)
        except AttributeError:
            return self.__default__(tree.data, children, tree.meta)
        else:
            try:
                wrapper = getattr(f, 'visit_wrapper', None)
                if wrapper is not None:
                    return f.visit_wrapper(f, tree.data, children, tree.meta)
                else:
                    return f(children)
            except (GrammarError, Discard):
                raise
            except Exception as e:
                raise VisitError(tree.data, tree, e)

    def _call_userfunc_token(self, token):
        try:
            f = getattr(self, token.type)
        except AttributeError:
            return self.__default_token__(token)
        else:
            try:
                return f(token)
            except (GrammarError, Discard):
                raise
            except Exception as e:
                raise VisitError(token.type, token, e)

    def _transform_children(self, children):
        for c in children:
            try:
                if isinstance(c, Tree):
                    yield self._transform_tree(c)
                elif self.__visit_tokens__ and isinstance(c, Token):
                    yield self._call_userfunc_token(c)
                else:
                    yield c
            except Discard:
                pass

    def _transform_tree(self, tree):
        children = list(self._transform_children(tree.children))
        return self._call_userfunc(tree, children)

    def transform(self, tree):
        #--
        return self._transform_tree(tree)

    def __mul__(self, other):
        #--
        return TransformerChain(self, other)

    def __default__(self, data, children, meta):
        #--
        return Tree(data, children, meta)

    def __default_token__(self, token):
        #--
        return token


class InlineTransformer(Transformer):   ##

    def _call_userfunc(self, tree, new_children=None):
        ##

        children = new_children if new_children is not None else tree.children
        try:
            f = getattr(self, tree.data)
        except AttributeError:
            return self.__default__(tree.data, children, tree.meta)
        else:
            return f(*children)


class TransformerChain(object):
    def __init__(self, *transformers):
        self.transformers = transformers

    def transform(self, tree):
        for t in self.transformers:
            tree = t.transform(tree)
        return tree

    def __mul__(self, other):
        return TransformerChain(*self.transformers + (other,))


class Transformer_InPlace(Transformer):
    #--
    def _transform_tree(self, tree):           ##

        return self._call_userfunc(tree)

    def transform(self, tree):
        for subtree in tree.iter_subtrees():
            subtree.children = list(self._transform_children(subtree.children))

        return self._transform_tree(tree)


class Transformer_NonRecursive(Transformer):
    #--

    def transform(self, tree):
        ##

        rev_postfix = []
        q = [tree]
        while q:
            t = q.pop()
            rev_postfix.append(t)
            if isinstance(t, Tree):
                q += t.children

        ##

        stack = []
        for x in reversed(rev_postfix):
            if isinstance(x, Tree):
                size = len(x.children)
                if size:
                    args = stack[-size:]
                    del stack[-size:]
                else:
                    args = []
                stack.append(self._call_userfunc(x, args))
            elif self.__visit_tokens__ and isinstance(x, Token):
                stack.append(self._call_userfunc_token(x))
            else:
                stack.append(x)

        t ,= stack  ##

        return t


class Transformer_InPlaceRecursive(Transformer):
    #--
    def _transform_tree(self, tree):
        tree.children = list(self._transform_children(tree.children))
        return self._call_userfunc(tree)


##


class VisitorBase:
    def _call_userfunc(self, tree):
        return getattr(self, tree.data, self.__default__)(tree)

    def __default__(self, tree):
        #--
        return tree

    def __class_getitem__(cls, _):
        return cls


class Visitor(VisitorBase):
    #--

    def visit(self, tree):
        #--
        for subtree in tree.iter_subtrees():
            self._call_userfunc(subtree)
        return tree

    def visit_topdown(self,tree):
        #--
        for subtree in tree.iter_subtrees_topdown():
            self._call_userfunc(subtree)
        return tree


class Visitor_Recursive(VisitorBase):
    #--

    def visit(self, tree):
        #--
        for child in tree.children:
            if isinstance(child, Tree):
                self.visit(child)

        self._call_userfunc(tree)
        return tree

    def visit_topdown(self,tree):
        #--
        self._call_userfunc(tree)

        for child in tree.children:
            if isinstance(child, Tree):
                self.visit_topdown(child)

        return tree


def visit_children_decor(func):
    #--
    @wraps(func)
    def inner(cls, tree):
        values = cls.visit_children(tree)
        return func(cls, values)
    return inner


class Interpreter(_Decoratable):
    #--

    def visit(self, tree):
        f = getattr(self, tree.data)
        wrapper = getattr(f, 'visit_wrapper', None)
        if wrapper is not None:
            return f.visit_wrapper(f, tree.data, tree.children, tree.meta)
        else:
            return f(tree)

    def visit_children(self, tree):
        return [self.visit(child) if isinstance(child, Tree) else child
                for child in tree.children]

    def __getattr__(self, name):
        return self.__default__

    def __default__(self, tree):
        return self.visit_children(tree)


##


def _apply_decorator(obj, decorator, **kwargs):
    try:
        _apply = obj._apply_decorator
    except AttributeError:
        return decorator(obj, **kwargs)
    else:
        return _apply(decorator, **kwargs)


def _inline_args__func(func):
    @wraps(func)
    def create_decorator(_f, with_self):
        if with_self:
            def f(self, children):
                return _f(self, *children)
        else:
            def f(self, children):
                return _f(*children)
        return f

    return smart_decorator(func, create_decorator)


def inline_args(obj):   ##

    return _apply_decorator(obj, _inline_args__func)


def _visitor_args_func_dec(func, visit_wrapper=None, static=False):
    def create_decorator(_f, with_self):
        if with_self:
            def f(self, *args, **kwargs):
                return _f(self, *args, **kwargs)
        else:
            def f(self, *args, **kwargs):
                return _f(*args, **kwargs)
        return f

    if static:
        f = wraps(func)(create_decorator(func, False))
    else:
        f = smart_decorator(func, create_decorator)
    f.vargs_applied = True
    f.visit_wrapper = visit_wrapper
    return f


def _vargs_inline(f, _data, children, _meta):
    return f(*children)
def _vargs_meta_inline(f, _data, children, meta):
    return f(meta, *children)
def _vargs_meta(f, _data, children, meta):
    return f(children, meta)   ##

def _vargs_tree(f, data, children, meta):
    return f(Tree(data, children, meta))


def v_args(inline=False, meta=False, tree=False, wrapper=None):
    #--
    if tree and (meta or inline):
        raise ValueError("Visitor functions cannot combine 'tree' with 'meta' or 'inline'.")

    func = None
    if meta:
        if inline:
            func = _vargs_meta_inline
        else:
            func = _vargs_meta
    elif inline:
        func = _vargs_inline
    elif tree:
        func = _vargs_tree

    if wrapper is not None:
        if func is not None:
            raise ValueError("Cannot use 'wrapper' along with 'tree', 'meta' or 'inline'.")
        func = wrapper

    def _visitor_args_dec(obj):
        return _apply_decorator(obj, _visitor_args_func_dec, visit_wrapper=func)
    return _visitor_args_dec




class Symbol(Serialize):
    __slots__ = ('name',)

    is_term = NotImplemented

    def __init__(self, name):
        self.name = name

    def __eq__(self, other):
        assert isinstance(other, Symbol), other
        return self.is_term == other.is_term and self.name == other.name

    def __ne__(self, other):
        return not (self == other)

    def __hash__(self):
        return hash(self.name)

    def __repr__(self):
        return '%s(%r)' % (type(self).__name__, self.name)

    fullrepr = property(__repr__)


class Terminal(Symbol):
    __serialize_fields__ = 'name', 'filter_out'

    is_term = True

    def __init__(self, name, filter_out=False):
        self.name = name
        self.filter_out = filter_out

    @property
    def fullrepr(self):
        return '%s(%r, %r)' % (type(self).__name__, self.name, self.filter_out)


class NonTerminal(Symbol):
    __serialize_fields__ = 'name',

    is_term = False


class RuleOptions(Serialize):
    __serialize_fields__ = 'keep_all_tokens', 'expand1', 'priority', 'template_source', 'empty_indices'

    def __init__(self, keep_all_tokens=False, expand1=False, priority=None, template_source=None, empty_indices=()):
        self.keep_all_tokens = keep_all_tokens
        self.expand1 = expand1
        self.priority = priority
        self.template_source = template_source
        self.empty_indices = empty_indices

    def __repr__(self):
        return 'RuleOptions(%r, %r, %r, %r)' % (
            self.keep_all_tokens,
            self.expand1,
            self.priority,
            self.template_source
        )


class Rule(Serialize):
    #--
    __slots__ = ('origin', 'expansion', 'alias', 'options', 'order', '_hash')

    __serialize_fields__ = 'origin', 'expansion', 'order', 'alias', 'options'
    __serialize_namespace__ = Terminal, NonTerminal, RuleOptions

    def __init__(self, origin, expansion, order=0, alias=None, options=None):
        self.origin = origin
        self.expansion = expansion
        self.alias = alias
        self.order = order
        self.options = options or RuleOptions()
        self._hash = hash((self.origin, tuple(self.expansion)))

    def _deserialize(self):
        self._hash = hash((self.origin, tuple(self.expansion)))

    def __str__(self):
        return '<%s : %s>' % (self.origin.name, ' '.join(x.name for x in self.expansion))

    def __repr__(self):
        return 'Rule(%r, %r, %r, %r)' % (self.origin, self.expansion, self.alias, self.options)

    def __hash__(self):
        return self._hash

    def __eq__(self, other):
        if not isinstance(other, Rule):
            return False
        return self.origin == other.origin and self.expansion == other.expansion



from copy import copy


class Pattern(Serialize):
    raw = None
    type = None

    def __init__(self, value, flags=(), raw=None):
        self.value = value
        self.flags = frozenset(flags)
        self.raw = raw

    def __repr__(self):
        return repr(self.to_regexp())

    ##

    def __hash__(self):
        return hash((type(self), self.value, self.flags))

    def __eq__(self, other):
        return type(self) == type(other) and self.value == other.value and self.flags == other.flags

    def to_regexp(self):
        raise NotImplementedError()

    def min_width(self):
        raise NotImplementedError()

    def max_width(self):
        raise NotImplementedError()

    if Py36:
        ##

        def _get_flags(self, value):
            for f in self.flags:
                value = ('(?%s:%s)' % (f, value))
            return value

    else:
        def _get_flags(self, value):
            for f in self.flags:
                value = ('(?%s)' % f) + value
            return value



class PatternStr(Pattern):
    __serialize_fields__ = 'value', 'flags'

    type = "str"

    def to_regexp(self):
        return self._get_flags(re.escape(self.value))

    @property
    def min_width(self):
        return len(self.value)
    max_width = min_width


class PatternRE(Pattern):
    __serialize_fields__ = 'value', 'flags', '_width'

    type = "re"

    def to_regexp(self):
        return self._get_flags(self.value)

    _width = None
    def _get_width(self):
        if self._width is None:
            self._width = get_regexp_width(self.to_regexp())
        return self._width

    @property
    def min_width(self):
        return self._get_width()[0]

    @property
    def max_width(self):
        return self._get_width()[1]


class TerminalDef(Serialize):
    __serialize_fields__ = 'name', 'pattern', 'priority'
    __serialize_namespace__ = PatternStr, PatternRE

    def __init__(self, name, pattern, priority=1):
        assert isinstance(pattern, Pattern), pattern
        self.name = name
        self.pattern = pattern
        self.priority = priority

    def __repr__(self):
        return '%s(%r, %r)' % (type(self).__name__, self.name, self.pattern)

    def user_repr(self):
        if self.name.startswith('__'): ##

            return self.pattern.raw or self.name
        else:
            return self.name


class Token(Str):
    #--
    __slots__ = ('type', 'pos_in_stream', 'value', 'line', 'column', 'end_line', 'end_column', 'end_pos')

    def __new__(cls, type_, value, pos_in_stream=None, line=None, column=None, end_line=None, end_column=None, end_pos=None):
        try:
            self = super(Token, cls).__new__(cls, value)
        except UnicodeDecodeError:
            value = value.decode('latin1')
            self = super(Token, cls).__new__(cls, value)

        self.type = type_
        self.pos_in_stream = pos_in_stream
        self.value = value
        self.line = line
        self.column = column
        self.end_line = end_line
        self.end_column = end_column
        self.end_pos = end_pos
        return self

    def update(self, type_=None, value=None):
        return Token.new_borrow_pos(
            type_ if type_ is not None else self.type,
            value if value is not None else self.value,
            self
        )

    @classmethod
    def new_borrow_pos(cls, type_, value, borrow_t):
        return cls(type_, value, borrow_t.pos_in_stream, borrow_t.line, borrow_t.column, borrow_t.end_line, borrow_t.end_column, borrow_t.end_pos)

    def __reduce__(self):
        return (self.__class__, (self.type, self.value, self.pos_in_stream, self.line, self.column))

    def __repr__(self):
        return 'Token(%r, %r)' % (self.type, self.value)

    def __deepcopy__(self, memo):
        return Token(self.type, self.value, self.pos_in_stream, self.line, self.column)

    def __eq__(self, other):
        if isinstance(other, Token) and self.type != other.type:
            return False

        return Str.__eq__(self, other)

    __hash__ = Str.__hash__


class LineCounter:
    __slots__ = 'char_pos', 'line', 'column', 'line_start_pos', 'newline_char'

    def __init__(self, newline_char):
        self.newline_char = newline_char
        self.char_pos = 0
        self.line = 1
        self.column = 1
        self.line_start_pos = 0

    def __eq__(self, other):
        if not isinstance(other, LineCounter):
            return NotImplemented

        return self.char_pos == other.char_pos and self.newline_char == other.newline_char

    def feed(self, token, test_newline=True):
        #--
        if test_newline:
            newlines = token.count(self.newline_char)
            if newlines:
                self.line += newlines
                self.line_start_pos = self.char_pos + token.rindex(self.newline_char) + 1

        self.char_pos += len(token)
        self.column = self.char_pos - self.line_start_pos + 1


class UnlessCallback:
    def __init__(self, mres):
        self.mres = mres

    def __call__(self, t):
        for mre, type_from_index in self.mres:
            m = mre.match(t.value)
            if m:
                t.type = type_from_index[m.lastindex]
                break
        return t


class CallChain:
    def __init__(self, callback1, callback2, cond):
        self.callback1 = callback1
        self.callback2 = callback2
        self.cond = cond

    def __call__(self, t):
        t2 = self.callback1(t)
        return self.callback2(t) if self.cond(t2) else t2


def _create_unless(terminals, g_regex_flags, re_, use_bytes):
    tokens_by_type = classify(terminals, lambda t: type(t.pattern))
    assert len(tokens_by_type) <= 2, tokens_by_type.keys()
    embedded_strs = set()
    callback = {}
    for retok in tokens_by_type.get(PatternRE, []):
        unless = []
        for strtok in tokens_by_type.get(PatternStr, []):
            if strtok.priority > retok.priority:
                continue
            s = strtok.pattern.value
            m = re_.match(retok.pattern.to_regexp(), s, g_regex_flags)
            if m and m.group(0) == s:
                unless.append(strtok)
                if strtok.pattern.flags <= retok.pattern.flags:
                    embedded_strs.add(strtok)
        if unless:
            callback[retok.name] = UnlessCallback(build_mres(unless, g_regex_flags, re_, match_whole=True, use_bytes=use_bytes))

    terminals = [t for t in terminals if t not in embedded_strs]
    return terminals, callback


def _build_mres(terminals, max_size, g_regex_flags, match_whole, re_, use_bytes):
    ##

    ##

    ##

    postfix = '$' if match_whole else ''
    mres = []
    while terminals:
        pattern = u'|'.join(u'(?P<%s>%s)' % (t.name, t.pattern.to_regexp() + postfix) for t in terminals[:max_size])
        if use_bytes:
            pattern = pattern.encode('latin-1')
        try:
            mre = re_.compile(pattern, g_regex_flags)
        except AssertionError:  ##

            return _build_mres(terminals, max_size//2, g_regex_flags, match_whole, re_, use_bytes)

        mres.append((mre, {i: n for n, i in mre.groupindex.items()}))
        terminals = terminals[max_size:]
    return mres


def build_mres(terminals, g_regex_flags, re_, use_bytes, match_whole=False):
    return _build_mres(terminals, len(terminals), g_regex_flags, match_whole, re_, use_bytes)


def _regexp_has_newline(r):
    #--
    return '\n' in r or '\\n' in r or '\\s' in r or '[^' in r or ('(?s' in r and '.' in r)


class Lexer(object):
    #--
    lex = NotImplemented

    def make_lexer_state(self, text):
        line_ctr = LineCounter(b'\n' if isinstance(text, bytes) else '\n')
        return LexerState(text, line_ctr)


class TraditionalLexer(Lexer):

    def __init__(self, conf):
        terminals = list(conf.terminals)
        assert all(isinstance(t, TerminalDef) for t in terminals), terminals

        self.re = conf.re_module

        if not conf.skip_validation:
            ##

            for t in terminals:
                try:
                    self.re.compile(t.pattern.to_regexp(), conf.g_regex_flags)
                except self.re.error:
                    raise LexError("Cannot compile token %s: %s" % (t.name, t.pattern))

                if t.pattern.min_width == 0:
                    raise LexError("Lexer does not allow zero-width terminals. (%s: %s)" % (t.name, t.pattern))

            if not (set(conf.ignore) <= {t.name for t in terminals}):
                raise LexError("Ignore terminals are not defined: %s" % (set(conf.ignore) - {t.name for t in terminals}))

        ##

        self.newline_types = frozenset(t.name for t in terminals if _regexp_has_newline(t.pattern.to_regexp()))
        self.ignore_types = frozenset(conf.ignore)

        terminals.sort(key=lambda x: (-x.priority, -x.pattern.max_width, -len(x.pattern.value), x.name))
        self.terminals = terminals
        self.user_callbacks = conf.callbacks
        self.g_regex_flags = conf.g_regex_flags
        self.use_bytes = conf.use_bytes
        self.terminals_by_name = conf.terminals_by_name

        self._mres = None

    def _build(self):
        terminals, self.callback = _create_unless(self.terminals, self.g_regex_flags, self.re, self.use_bytes)
        assert all(self.callback.values())

        for type_, f in self.user_callbacks.items():
            if type_ in self.callback:
                ##

                self.callback[type_] = CallChain(self.callback[type_], f, lambda t: t.type == type_)
            else:
                self.callback[type_] = f

        self._mres = build_mres(terminals, self.g_regex_flags, self.re, self.use_bytes)

    @property
    def mres(self):
        if self._mres is None:
            self._build()
        return self._mres

    def match(self, text, pos):
        for mre, type_from_index in self.mres:
            m = mre.match(text, pos)
            if m:
                return m.group(0), type_from_index[m.lastindex]

    def lex(self, state, parser_state):
        with suppress(EOFError):
            while True:
                yield self.next_token(state, parser_state)

    def next_token(self, lex_state, parser_state=None):
        line_ctr = lex_state.line_ctr
        while line_ctr.char_pos < len(lex_state.text):
            res = self.match(lex_state.text, line_ctr.char_pos)
            if not res:
                allowed = {v for m, tfi in self.mres for v in tfi.values()} - self.ignore_types
                if not allowed:
                    allowed = {"<END-OF-FILE>"}
                raise UnexpectedCharacters(lex_state.text, line_ctr.char_pos, line_ctr.line, line_ctr.column,
                                           allowed=allowed, token_history=lex_state.last_token and [lex_state.last_token],
                                           state=parser_state, terminals_by_name=self.terminals_by_name)

            value, type_ = res

            if type_ not in self.ignore_types:
                t = Token(type_, value, line_ctr.char_pos, line_ctr.line, line_ctr.column)
                line_ctr.feed(value, type_ in self.newline_types)
                t.end_line = line_ctr.line
                t.end_column = line_ctr.column
                t.end_pos = line_ctr.char_pos
                if t.type in self.callback:
                    t = self.callback[t.type](t)
                    if not isinstance(t, Token):
                        raise LexError("Callbacks must return a token (returned %r)" % t)
                lex_state.last_token = t
                return t
            else:
                if type_ in self.callback:
                    t2 = Token(type_, value, line_ctr.char_pos, line_ctr.line, line_ctr.column)
                    self.callback[type_](t2)
                line_ctr.feed(value, type_ in self.newline_types)

        ##

        raise EOFError(self)


class LexerState(object):
    __slots__ = 'text', 'line_ctr', 'last_token'

    def __init__(self, text, line_ctr, last_token=None):
        self.text = text
        self.line_ctr = line_ctr
        self.last_token = last_token

    def __eq__(self, other):
        if not isinstance(other, LexerState):
            return NotImplemented

        return self.text is other.text and self.line_ctr == other.line_ctr and self.last_token == other.last_token

    def __copy__(self):
        return type(self)(self.text, copy(self.line_ctr), self.last_token)


class ContextualLexer(Lexer):

    def __init__(self, conf, states, always_accept=()):
        terminals = list(conf.terminals)
        terminals_by_name = conf.terminals_by_name

        trad_conf = copy(conf)
        trad_conf.terminals = terminals

        lexer_by_tokens = {}
        self.lexers = {}
        for state, accepts in states.items():
            key = frozenset(accepts)
            try:
                lexer = lexer_by_tokens[key]
            except KeyError:
                accepts = set(accepts) | set(conf.ignore) | set(always_accept)
                lexer_conf = copy(trad_conf)
                lexer_conf.terminals = [terminals_by_name[n] for n in accepts if n in terminals_by_name]
                lexer = TraditionalLexer(lexer_conf)
                lexer_by_tokens[key] = lexer

            self.lexers[state] = lexer

        assert trad_conf.terminals is terminals
        self.root_lexer = TraditionalLexer(trad_conf)

    def make_lexer_state(self, text):
        return self.root_lexer.make_lexer_state(text)

    def lex(self, lexer_state, parser_state):
        try:
            while True:
                lexer = self.lexers[parser_state.position]
                yield lexer.next_token(lexer_state, parser_state)
        except EOFError:
            pass
        except UnexpectedCharacters as e:
            ##

            ##

            try:
                last_token = lexer_state.last_token  ##

                token = self.root_lexer.next_token(lexer_state, parser_state)
                raise UnexpectedToken(token, e.allowed, state=parser_state, token_history=[last_token], terminals_by_name=self.root_lexer.terminals_by_name)
            except UnexpectedCharacters:
                raise e  ##


class LexerThread(object):
    #--

    def __init__(self, lexer, text):
        self.lexer = lexer
        self.state = lexer.make_lexer_state(text)

    def lex(self, parser_state):
        return self.lexer.lex(self.state, parser_state)

    def __copy__(self):
        copied = object.__new__(LexerThread)
        copied.lexer = self.lexer
        copied.state = copy(self.state)
        return copied



class LexerConf(Serialize):
    __serialize_fields__ = 'terminals', 'ignore', 'g_regex_flags', 'use_bytes', 'lexer_type'
    __serialize_namespace__ = TerminalDef,

    def __init__(self, terminals, re_module, ignore=(), postlex=None, callbacks=None, g_regex_flags=0, skip_validation=False, use_bytes=False):
        self.terminals = terminals
        self.terminals_by_name = {t.name: t for t in self.terminals}
        assert len(self.terminals) == len(self.terminals_by_name)
        self.ignore = ignore
        self.postlex = postlex
        self.callbacks = callbacks or {}
        self.g_regex_flags = g_regex_flags
        self.re_module = re_module
        self.skip_validation = skip_validation
        self.use_bytes = use_bytes
        self.lexer_type = None

    @property
    def tokens(self):
        warn("LexerConf.tokens is deprecated. Use LexerConf.terminals instead", DeprecationWarning)
        return self.terminals

    def _deserialize(self):
        self.terminals_by_name = {t.name: t for t in self.terminals}



class ParserConf(Serialize):
    __serialize_fields__ = 'rules', 'start', 'parser_type'

    def __init__(self, rules, callbacks, start):
        assert isinstance(start, list)
        self.rules = rules
        self.callbacks = callbacks
        self.start = start

        self.parser_type = None


from functools import partial, wraps
from itertools import repeat, product


class ExpandSingleChild:
    def __init__(self, node_builder):
        self.node_builder = node_builder

    def __call__(self, children):
        if len(children) == 1:
            return children[0]
        else:
            return self.node_builder(children)


class PropagatePositions:
    def __init__(self, node_builder):
        self.node_builder = node_builder

    def __call__(self, children):
        res = self.node_builder(children)

        ##

        if isinstance(res, Tree):
            res_meta = res.meta
            for c in children:
                if isinstance(c, Tree):
                    child_meta = c.meta
                    if not child_meta.empty:
                        res_meta.line = child_meta.line
                        res_meta.column = child_meta.column
                        res_meta.start_pos = child_meta.start_pos
                        res_meta.empty = False
                        break
                elif isinstance(c, Token):
                    res_meta.line = c.line
                    res_meta.column = c.column
                    res_meta.start_pos = c.pos_in_stream
                    res_meta.empty = False
                    break

            for c in reversed(children):
                if isinstance(c, Tree):
                    child_meta = c.meta
                    if not child_meta.empty:
                        res_meta.end_line = child_meta.end_line
                        res_meta.end_column = child_meta.end_column
                        res_meta.end_pos = child_meta.end_pos
                        res_meta.empty = False
                        break
                elif isinstance(c, Token):
                    res_meta.end_line = c.end_line
                    res_meta.end_column = c.end_column
                    res_meta.end_pos = c.end_pos
                    res_meta.empty = False
                    break

        return res


class ChildFilter:
    def __init__(self, to_include, append_none, node_builder):
        self.node_builder = node_builder
        self.to_include = to_include
        self.append_none = append_none

    def __call__(self, children):
        filtered = []

        for i, to_expand, add_none in self.to_include:
            if add_none:
                filtered += [None] * add_none
            if to_expand:
                filtered += children[i].children
            else:
                filtered.append(children[i])

        if self.append_none:
            filtered += [None] * self.append_none

        return self.node_builder(filtered)


class ChildFilterLALR(ChildFilter):
    #--

    def __call__(self, children):
        filtered = []
        for i, to_expand, add_none in self.to_include:
            if add_none:
                filtered += [None] * add_none
            if to_expand:
                if filtered:
                    filtered += children[i].children
                else:   ##

                    filtered = children[i].children
            else:
                filtered.append(children[i])

        if self.append_none:
            filtered += [None] * self.append_none

        return self.node_builder(filtered)


class ChildFilterLALR_NoPlaceholders(ChildFilter):
    #--
    def __init__(self, to_include, node_builder):
        self.node_builder = node_builder
        self.to_include = to_include

    def __call__(self, children):
        filtered = []
        for i, to_expand in self.to_include:
            if to_expand:
                if filtered:
                    filtered += children[i].children
                else:   ##

                    filtered = children[i].children
            else:
                filtered.append(children[i])
        return self.node_builder(filtered)


def _should_expand(sym):
    return not sym.is_term and sym.name.startswith('_')


def maybe_create_child_filter(expansion, keep_all_tokens, ambiguous, _empty_indices):
    ##

    if _empty_indices:
        assert _empty_indices.count(False) == len(expansion)
        s = ''.join(str(int(b)) for b in _empty_indices)
        empty_indices = [len(ones) for ones in s.split('0')]
        assert len(empty_indices) == len(expansion)+1, (empty_indices, len(expansion))
    else:
        empty_indices = [0] * (len(expansion)+1)

    to_include = []
    nones_to_add = 0
    for i, sym in enumerate(expansion):
        nones_to_add += empty_indices[i]
        if keep_all_tokens or not (sym.is_term and sym.filter_out):
            to_include.append((i, _should_expand(sym), nones_to_add))
            nones_to_add = 0

    nones_to_add += empty_indices[len(expansion)]

    if _empty_indices or len(to_include) < len(expansion) or any(to_expand for i, to_expand,_ in to_include):
        if _empty_indices or ambiguous:
            return partial(ChildFilter if ambiguous else ChildFilterLALR, to_include, nones_to_add)
        else:
            ##

            return partial(ChildFilterLALR_NoPlaceholders, [(i, x) for i,x,_ in to_include])


class AmbiguousExpander:
    #--
    def __init__(self, to_expand, tree_class, node_builder):
        self.node_builder = node_builder
        self.tree_class = tree_class
        self.to_expand = to_expand

    def __call__(self, children):
        def _is_ambig_tree(t):
            return hasattr(t, 'data') and t.data == '_ambig'

        ##

        ##

        ##

        ##

        ambiguous = []
        for i, child in enumerate(children):
            if _is_ambig_tree(child):
                if i in self.to_expand:
                    ambiguous.append(i)

                to_expand = [j for j, grandchild in enumerate(child.children) if _is_ambig_tree(grandchild)]
                child.expand_kids_by_index(*to_expand)

        if not ambiguous:
            return self.node_builder(children)

        expand = [iter(child.children) if i in ambiguous else repeat(child) for i, child in enumerate(children)]
        return self.tree_class('_ambig', [self.node_builder(list(f[0])) for f in product(zip(*expand))])


def maybe_create_ambiguous_expander(tree_class, expansion, keep_all_tokens):
    to_expand = [i for i, sym in enumerate(expansion)
                 if keep_all_tokens or ((not (sym.is_term and sym.filter_out)) and _should_expand(sym))]
    if to_expand:
        return partial(AmbiguousExpander, to_expand, tree_class)


class AmbiguousIntermediateExpander:
    #--

    def __init__(self, tree_class, node_builder):
        self.node_builder = node_builder
        self.tree_class = tree_class

    def __call__(self, children):
        def _is_iambig_tree(child):
            return hasattr(child, 'data') and child.data == '_iambig'

        def _collapse_iambig(children):
            #--

            ##

            ##

            if children and _is_iambig_tree(children[0]):
                iambig_node = children[0]
                result = []
                for grandchild in iambig_node.children:
                    collapsed = _collapse_iambig(grandchild.children)
                    if collapsed:
                        for child in collapsed:
                            child.children += children[1:]
                        result += collapsed
                    else:
                        new_tree = self.tree_class('_inter', grandchild.children + children[1:])
                        result.append(new_tree)
                return result

        collapsed = _collapse_iambig(children)
        if collapsed:
            processed_nodes = [self.node_builder(c.children) for c in collapsed]
            return self.tree_class('_ambig', processed_nodes)

        return self.node_builder(children)


def ptb_inline_args(func):
    @wraps(func)
    def f(children):
        return func(*children)
    return f


def inplace_transformer(func):
    @wraps(func)
    def f(children):
        ##

        tree = Tree(func.__name__, children)
        return func(tree)
    return f


def apply_visit_wrapper(func, name, wrapper):
    if wrapper is _vargs_meta or wrapper is _vargs_meta_inline:
        raise NotImplementedError("Meta args not supported for internal transformer")

    @wraps(func)
    def f(children):
        return wrapper(func, name, children, None)
    return f


class ParseTreeBuilder:
    def __init__(self, rules, tree_class, propagate_positions=False, ambiguous=False, maybe_placeholders=False):
        self.tree_class = tree_class
        self.propagate_positions = propagate_positions
        self.ambiguous = ambiguous
        self.maybe_placeholders = maybe_placeholders

        self.rule_builders = list(self._init_builders(rules))

    def _init_builders(self, rules):
        for rule in rules:
            options = rule.options
            keep_all_tokens = options.keep_all_tokens
            expand_single_child = options.expand1

            wrapper_chain = list(filter(None, [
                (expand_single_child and not rule.alias) and ExpandSingleChild,
                maybe_create_child_filter(rule.expansion, keep_all_tokens, self.ambiguous, options.empty_indices if self.maybe_placeholders else None),
                self.propagate_positions and PropagatePositions,
                self.ambiguous and maybe_create_ambiguous_expander(self.tree_class, rule.expansion, keep_all_tokens),
                self.ambiguous and partial(AmbiguousIntermediateExpander, self.tree_class)
            ]))

            yield rule, wrapper_chain

    def create_callback(self, transformer=None):
        callbacks = {}

        for rule, wrapper_chain in self.rule_builders:

            user_callback_name = rule.alias or rule.options.template_source or rule.origin.name
            try:
                f = getattr(transformer, user_callback_name)
                ##

                wrapper = getattr(f, 'visit_wrapper', None)
                if wrapper is not None:
                    f = apply_visit_wrapper(f, user_callback_name, wrapper)
                else:
                    if isinstance(transformer, InlineTransformer):
                        f = ptb_inline_args(f)
                    elif isinstance(transformer, Transformer_InPlace):
                        f = inplace_transformer(f)
            except AttributeError:
                f = partial(self.tree_class, user_callback_name)

            for w in wrapper_chain:
                f = w(f)

            if rule in callbacks:
                raise GrammarError("Rule '%s' already exists" % (rule,))

            callbacks[rule] = f

        return callbacks



class LALR_Parser(Serialize):
    def __init__(self, parser_conf, debug=False):
        analysis = LALR_Analyzer(parser_conf, debug=debug)
        analysis.compute_lalr()
        callbacks = parser_conf.callbacks

        self._parse_table = analysis.parse_table
        self.parser_conf = parser_conf
        self.parser = _Parser(analysis.parse_table, callbacks, debug)

    @classmethod
    def deserialize(cls, data, memo, callbacks, debug=False):
        inst = cls.__new__(cls)
        inst._parse_table = IntParseTable.deserialize(data, memo)
        inst.parser = _Parser(inst._parse_table, callbacks, debug)
        return inst

    def serialize(self, memo):
        return self._parse_table.serialize(memo)
    
    def parse_interactive(self, lexer, start):
        return self.parser.parse(lexer, start, start_interactive=True)

    def parse(self, lexer, start, on_error=None):
        try:
            return self.parser.parse(lexer, start)
        except UnexpectedInput as e:
            if on_error is None:
                raise

            while True:
                if isinstance(e, UnexpectedCharacters):
                    s = e.interactive_parser.lexer_state.state
                    p = s.line_ctr.char_pos

                if not on_error(e):
                    raise e

                if isinstance(e, UnexpectedCharacters):
                    ##

                    if p == s.line_ctr.char_pos:
                        s.line_ctr.feed(s.text[p:p+1])

                try:
                    return e.interactive_parser.resume_parse()
                except UnexpectedToken as e2:
                    if (isinstance(e, UnexpectedToken)
                        and e.token.type == e2.token.type == '$END'
                        and e.interactive_parser == e2.interactive_parser):
                        ##

                        raise e2
                    e = e2
                except UnexpectedCharacters as e2:
                    e = e2


class ParseConf(object):
    __slots__ = 'parse_table', 'callbacks', 'start', 'start_state', 'end_state', 'states'

    def __init__(self, parse_table, callbacks, start):
        self.parse_table = parse_table

        self.start_state = self.parse_table.start_states[start]
        self.end_state = self.parse_table.end_states[start]
        self.states = self.parse_table.states

        self.callbacks = callbacks
        self.start = start


class ParserState(object):
    __slots__ = 'parse_conf', 'lexer', 'state_stack', 'value_stack'

    def __init__(self, parse_conf, lexer, state_stack=None, value_stack=None):
        self.parse_conf = parse_conf
        self.lexer = lexer
        self.state_stack = state_stack or [self.parse_conf.start_state]
        self.value_stack = value_stack or []

    @property
    def position(self):
        return self.state_stack[-1]

    ##

    def __eq__(self, other):
        if not isinstance(other, ParserState):
            return NotImplemented
        return len(self.state_stack) == len(other.state_stack) and self.position == other.position

    def __copy__(self):
        return type(self)(
            self.parse_conf,
            self.lexer, ##

            copy(self.state_stack),
            deepcopy(self.value_stack),
        )

    def copy(self):
        return copy(self)

    def feed_token(self, token, is_end=False):
        state_stack = self.state_stack
        value_stack = self.value_stack
        states = self.parse_conf.states
        end_state = self.parse_conf.end_state
        callbacks = self.parse_conf.callbacks

        while True:
            state = state_stack[-1]
            try:
                action, arg = states[state][token.type]
            except KeyError:
                expected = {s for s in states[state].keys() if s.isupper()}
                raise UnexpectedToken(token, expected, state=self, interactive_parser=None)

            assert arg != end_state

            if action is Shift:
                ##

                assert not is_end
                state_stack.append(arg)
                value_stack.append(token if token.type not in callbacks else callbacks[token.type](token))
                return
            else:
                ##

                rule = arg
                size = len(rule.expansion)
                if size:
                    s = value_stack[-size:]
                    del state_stack[-size:]
                    del value_stack[-size:]
                else:
                    s = []

                value = callbacks[rule](s)

                _action, new_state = states[state_stack[-1]][rule.origin.name]
                assert _action is Shift
                state_stack.append(new_state)
                value_stack.append(value)

                if is_end and state_stack[-1] == end_state:
                    return value_stack[-1]

class _Parser(object):
    def __init__(self, parse_table, callbacks, debug=False):
        self.parse_table = parse_table
        self.callbacks = callbacks
        self.debug = debug

    def parse(self, lexer, start, value_stack=None, state_stack=None, start_interactive=False):
        parse_conf = ParseConf(self.parse_table, self.callbacks, start)
        parser_state = ParserState(parse_conf, lexer, state_stack, value_stack)
        if start_interactive:
            return InteractiveParser(self, parser_state, parser_state.lexer)
        return self.parse_from_state(parser_state)
    

    def parse_from_state(self, state):
        ##

        try:
            token = None
            for token in state.lexer.lex(state):
                state.feed_token(token)

            token = Token.new_borrow_pos('$END', '', token) if token else Token('$END', '', 0, 1, 1)
            return state.feed_token(token, True)
        except UnexpectedInput as e:
            try:
                e.interactive_parser = InteractiveParser(self, state, state.lexer)
            except NameError:
                pass
            raise e
        except Exception as e:
            if self.debug:
                print("")
                print("STATE STACK DUMP")
                print("----------------")
                for i, s in enumerate(state.state_stack):
                    print('%d)' % i , s)
                print("")

            raise


class Action:
    def __init__(self, name):
        self.name = name
    def __str__(self):
        return self.name
    def __repr__(self):
        return str(self)

Shift = Action('Shift')
Reduce = Action('Reduce')


class ParseTable:
    def __init__(self, states, start_states, end_states):
        self.states = states
        self.start_states = start_states
        self.end_states = end_states

    def serialize(self, memo):
        tokens = Enumerator()
        rules = Enumerator()

        states = {
            state: {tokens.get(token): ((1, arg.serialize(memo)) if action is Reduce else (0, arg))
                    for token, (action, arg) in actions.items()}
            for state, actions in self.states.items()
        }

        return {
            'tokens': tokens.reversed(),
            'states': states,
            'start_states': self.start_states,
            'end_states': self.end_states,
        }

    @classmethod
    def deserialize(cls, data, memo):
        tokens = data['tokens']
        states = {
            state: {tokens[token]: ((Reduce, Rule.deserialize(arg, memo)) if action==1 else (Shift, arg))
                    for token, (action, arg) in actions.items()}
            for state, actions in data['states'].items()
        }
        return cls(states, data['start_states'], data['end_states'])


class IntParseTable(ParseTable):

    @classmethod
    def from_ParseTable(cls, parse_table):
        enum = list(parse_table.states)
        state_to_idx = {s:i for i,s in enumerate(enum)}
        int_states = {}

        for s, la in parse_table.states.items():
            la = {k:(v[0], state_to_idx[v[1]]) if v[0] is Shift else v
                  for k,v in la.items()}
            int_states[ state_to_idx[s] ] = la


        start_states = {start:state_to_idx[s] for start, s in parse_table.start_states.items()}
        end_states = {start:state_to_idx[s] for start, s in parse_table.end_states.items()}
        return cls(int_states, start_states, end_states)



def _wrap_lexer(lexer_class):
    future_interface = getattr(lexer_class, '__future_interface__', False)
    if future_interface:
        return lexer_class
    else:
        class CustomLexerWrapper(Lexer):
            def __init__(self, lexer_conf):
                self.lexer = lexer_class(lexer_conf)
            def lex(self, lexer_state, parser_state):
                return self.lexer.lex(lexer_state.text)
        return CustomLexerWrapper


class MakeParsingFrontend:
    def __init__(self, parser_type, lexer_type):
        self.parser_type = parser_type
        self.lexer_type = lexer_type

    def __call__(self, lexer_conf, parser_conf, options):
        assert isinstance(lexer_conf, LexerConf)
        assert isinstance(parser_conf, ParserConf)
        parser_conf.parser_type = self.parser_type
        lexer_conf.lexer_type = self.lexer_type
        return ParsingFrontend(lexer_conf, parser_conf, options)

    @classmethod
    def deserialize(cls, data, memo, lexer_conf, callbacks, options):
        parser_conf = ParserConf.deserialize(data['parser_conf'], memo)
        parser = LALR_Parser.deserialize(data['parser'], memo, callbacks, options.debug)
        parser_conf.callbacks = callbacks
        return ParsingFrontend(lexer_conf, parser_conf, options, parser=parser)




class ParsingFrontend(Serialize):
    __serialize_fields__ = 'lexer_conf', 'parser_conf', 'parser', 'options'

    def __init__(self, lexer_conf, parser_conf, options, parser=None):
        self.parser_conf = parser_conf
        self.lexer_conf = lexer_conf
        self.options = options

        ##

        if parser:  ##

            self.parser = parser
        else:
            create_parser = {
                'lalr': create_lalr_parser,
                'earley': create_earley_parser,
                'cyk': CYK_FrontEnd,
            }[parser_conf.parser_type]
            self.parser = create_parser(lexer_conf, parser_conf, options)

        ##

        lexer_type = lexer_conf.lexer_type
        self.skip_lexer = False
        if lexer_type in ('dynamic', 'dynamic_complete'):
            assert lexer_conf.postlex is None
            self.skip_lexer = True
            return

        try:
            create_lexer = {
                'standard': create_traditional_lexer,
                'contextual': create_contextual_lexer,
            }[lexer_type]
        except KeyError:
            assert issubclass(lexer_type, Lexer), lexer_type
            self.lexer = _wrap_lexer(lexer_type)(lexer_conf)
        else:
            self.lexer = create_lexer(lexer_conf, self.parser, lexer_conf.postlex)

        if lexer_conf.postlex:
            self.lexer = PostLexConnector(self.lexer, lexer_conf.postlex)
    
    def _verify_start(self, start=None):
        if start is None:
            start = self.parser_conf.start
            if len(start) > 1:
                raise ConfigurationError("Lark initialized with more than 1 possible start rule. Must specify which start rule to parse", start)
            start ,= start
        elif start not in self.parser_conf.start:
            raise ConfigurationError("Unknown start rule %s. Must be one of %r" % (start, self.parser_conf.start))
        return start

    def parse(self, text, start=None, on_error=None):
        start = self._verify_start(start)
        stream = text if self.skip_lexer else LexerThread(self.lexer, text)
        kw = {} if on_error is None else {'on_error': on_error}
        return self.parser.parse(stream, start, **kw)
    
    def parse_interactive(self, text=None, start=None):
        start = self._verify_start(start)
        if self.parser_conf.parser_type != 'lalr':
            raise ConfigurationError("parse_interactive() currently only works with parser='lalr' ")
        stream = text if self.skip_lexer else LexerThread(self.lexer, text)
        return self.parser.parse_interactive(stream, start)


def get_frontend(parser, lexer):
    assert_config(parser, ('lalr', 'earley', 'cyk'))
    if not isinstance(lexer, type):     ##

        expected = {
            'lalr': ('standard', 'contextual'),
            'earley': ('standard', 'dynamic', 'dynamic_complete'),
            'cyk': ('standard', ),
         }[parser]
        assert_config(lexer, expected, 'Parser %r does not support lexer %%r, expected one of %%s' % parser)

    return MakeParsingFrontend(parser, lexer)


def _get_lexer_callbacks(transformer, terminals):
    result = {}
    for terminal in terminals:
        callback = getattr(transformer, terminal.name, None)
        if callback is not None:
            result[terminal.name] = callback
    return result

class PostLexConnector:
    def __init__(self, lexer, postlexer):
        self.lexer = lexer
        self.postlexer = postlexer

    def make_lexer_state(self, text):
        return self.lexer.make_lexer_state(text)

    def lex(self, lexer_state, parser_state):
        i = self.lexer.lex(lexer_state, parser_state)
        return self.postlexer.process(i)



def create_traditional_lexer(lexer_conf, parser, postlex):
    return TraditionalLexer(lexer_conf)

def create_contextual_lexer(lexer_conf, parser, postlex):
    states = {idx:list(t.keys()) for idx, t in parser._parse_table.states.items()}
    always_accept = postlex.always_accept if postlex else ()
    return ContextualLexer(lexer_conf, states, always_accept=always_accept)

def create_lalr_parser(lexer_conf, parser_conf, options=None):
    debug = options.debug if options else False
    return LALR_Parser(parser_conf, debug=debug)


create_earley_parser = NotImplemented
CYK_FrontEnd = NotImplemented



class LarkOptions(Serialize):
    #--
    OPTIONS_DOC = """
    **===  General Options  ===**

    start
            The start symbol. Either a string, or a list of strings for multiple possible starts (Default: "start")
    debug
            Display debug information and extra warnings. Use only when debugging (default: False)
            When used with Earley, it generates a forest graph as "sppf.png", if 'dot' is installed.
    transformer
            Applies the transformer to every parse tree (equivalent to applying it after the parse, but faster)
    propagate_positions
            Propagates (line, column, end_line, end_column) attributes into all tree branches.
    maybe_placeholders
            When True, the ``[]`` operator returns ``None`` when not matched.

            When ``False``,  ``[]`` behaves like the ``?`` operator, and returns no value at all.
            (default= ``False``. Recommended to set to ``True``)
    cache
            Cache the results of the Lark grammar analysis, for x2 to x3 faster loading. LALR only for now.

            - When ``False``, does nothing (default)
            - When ``True``, caches to a temporary file in the local directory
            - When given a string, caches to the path pointed by the string
    regex
            When True, uses the ``regex`` module instead of the stdlib ``re``.
    g_regex_flags
            Flags that are applied to all terminals (both regex and strings)
    keep_all_tokens
            Prevent the tree builder from automagically removing "punctuation" tokens (default: False)
    tree_class
            Lark will produce trees comprised of instances of this class instead of the default ``lark.Tree``.

    **=== Algorithm Options ===**

    parser
            Decides which parser engine to use. Accepts "earley" or "lalr". (Default: "earley").
            (there is also a "cyk" option for legacy)
    lexer
            Decides whether or not to use a lexer stage

            - "auto" (default): Choose for me based on the parser
            - "standard": Use a standard lexer
            - "contextual": Stronger lexer (only works with parser="lalr")
            - "dynamic": Flexible and powerful (only with parser="earley")
            - "dynamic_complete": Same as dynamic, but tries *every* variation of tokenizing possible.
    ambiguity
            Decides how to handle ambiguity in the parse. Only relevant if parser="earley"

            - "resolve": The parser will automatically choose the simplest derivation
              (it chooses consistently: greedy for tokens, non-greedy for rules)
            - "explicit": The parser will return all derivations wrapped in "_ambig" tree nodes (i.e. a forest).
            - "forest": The parser will return the root of the shared packed parse forest.

    **=== Misc. / Domain Specific Options ===**

    postlex
            Lexer post-processing (Default: None) Only works with the standard and contextual lexers.
    priority
            How priorities should be evaluated - auto, none, normal, invert (Default: auto)
    lexer_callbacks
            Dictionary of callbacks for the lexer. May alter tokens during lexing. Use with caution.
    use_bytes
            Accept an input of type ``bytes`` instead of ``str`` (Python 3 only).
    edit_terminals
            A callback for editing the terminals before parse.
    import_paths
            A List of either paths or loader functions to specify from where grammars are imported
    source_path
            Override the source of from where the grammar was loaded. Useful for relative imports and unconventional grammar loading
    **=== End Options ===**
    """
    if __doc__:
        __doc__ += OPTIONS_DOC


    ##

    ##

    ##

    ##

    ##

    ##

    ##

    ##

    _defaults = {
        'debug': False,
        'keep_all_tokens': False,
        'tree_class': None,
        'cache': False,
        'postlex': None,
        'parser': 'earley',
        'lexer': 'auto',
        'transformer': None,
        'start': 'start',
        'priority': 'auto',
        'ambiguity': 'auto',
        'regex': False,
        'propagate_positions': False,
        'lexer_callbacks': {},
        'maybe_placeholders': False,
        'edit_terminals': None,
        'g_regex_flags': 0,
        'use_bytes': False,
        'import_paths': [],
        'source_path': None,
    }

    def __init__(self, options_dict):
        o = dict(options_dict)

        options = {}
        for name, default in self._defaults.items():
            if name in o:
                value = o.pop(name)
                if isinstance(default, bool) and name not in ('cache', 'use_bytes'):
                    value = bool(value)
            else:
                value = default

            options[name] = value

        if isinstance(options['start'], STRING_TYPE):
            options['start'] = [options['start']]

        self.__dict__['options'] = options


        assert_config(self.parser, ('earley', 'lalr', 'cyk', None))

        if self.parser == 'earley' and self.transformer:
            raise ConfigurationError('Cannot specify an embedded transformer when using the Earley algorithm.'
                             'Please use your transformer on the resulting parse tree, or use a different algorithm (i.e. LALR)')

        if o:
            raise ConfigurationError("Unknown options: %s" % o.keys())

    def __getattr__(self, name):
        try:
            return self.__dict__['options'][name]
        except KeyError as e:
            raise AttributeError(e)

    def __setattr__(self, name, value):
        assert_config(name, self.options.keys(), "%r isn't a valid option. Expected one of: %s")
        self.options[name] = value

    def serialize(self, memo):
        return self.options

    @classmethod
    def deserialize(cls, data, memo):
        return cls(data)


##

##

_LOAD_ALLOWED_OPTIONS = {'postlex', 'transformer', 'lexer_callbacks', 'use_bytes', 'debug', 'g_regex_flags', 'regex', 'propagate_positions', 'tree_class'}

_VALID_PRIORITY_OPTIONS = ('auto', 'normal', 'invert', None)
_VALID_AMBIGUITY_OPTIONS = ('auto', 'resolve', 'explicit', 'forest')


class PostLex(ABC):
    @abstractmethod
    def process(self, stream):
        return stream

    always_accept = ()


class Lark(Serialize):
    #--
    def __init__(self, grammar, **options):
        self.options = LarkOptions(options)

        ##

        use_regex = self.options.regex
        if use_regex:
            if regex:
                re_module = regex
            else:
                raise ImportError('`regex` module must be installed if calling `Lark(regex=True)`.')
        else:
            re_module = re

        ##

        if self.options.source_path is None:
            try:
                self.source_path = grammar.name
            except AttributeError:
                self.source_path = '<string>'
        else:
            self.source_path = self.options.source_path

        ##

        try:
            read = grammar.read
        except AttributeError:
            pass
        else:
            grammar = read()

        cache_fn = None
        cache_md5 = None
        if isinstance(grammar, STRING_TYPE):
            self.source_grammar = grammar
            if self.options.use_bytes:
                if not isascii(grammar):
                    raise ConfigurationError("Grammar must be ascii only, when use_bytes=True")
                if sys.version_info[0] == 2 and self.options.use_bytes != 'force':
                    raise ConfigurationError("`use_bytes=True` may have issues on python2."
                                              "Use `use_bytes='force'` to use it at your own risk.")

            if self.options.cache:
                if self.options.parser != 'lalr':
                    raise ConfigurationError("cache only works with parser='lalr' for now")

                unhashable = ('transformer', 'postlex', 'lexer_callbacks', 'edit_terminals')
                options_str = ''.join(k+str(v) for k, v in options.items() if k not in unhashable)
                from . import __version__
                s = grammar + options_str + __version__ + str(sys.version_info[:2])
                cache_md5 = hashlib.md5(s.encode('utf8')).hexdigest()

                if isinstance(self.options.cache, STRING_TYPE):
                    cache_fn = self.options.cache
                else:
                    if self.options.cache is not True:
                        raise ConfigurationError("cache argument must be bool or str")
                    ##

                    cache_fn = tempfile.gettempdir() + '/.lark_cache_%s_%s_%s.tmp' % ((cache_md5,) + sys.version_info[:2])

                if FS.exists(cache_fn):
                    logger.debug('Loading grammar from cache: %s', cache_fn)
                    ##

                    for name in (set(options) - _LOAD_ALLOWED_OPTIONS):
                        del options[name]
                    with FS.open(cache_fn, 'rb') as f:
                        old_options = self.options
                        try:
                            file_md5 = f.readline().rstrip(b'\n')
                            cached_used_files = pickle.load(f)
                            if file_md5 == cache_md5.encode('utf8') and verify_used_files(cached_used_files):
                                cached_parser_data = pickle.load(f)
                                self._load(cached_parser_data, **options)
                                return
                        except Exception: ##

                            logger.exception("Failed to load Lark from cache: %r. We will try to carry on." % cache_fn)
                            
                            ##

                            ##

                            self.options = old_options


            ##

            self.grammar, used_files = load_grammar(grammar, self.source_path, self.options.import_paths, self.options.keep_all_tokens)
        else:
            assert isinstance(grammar, Grammar)
            self.grammar = grammar


        if self.options.lexer == 'auto':
            if self.options.parser == 'lalr':
                self.options.lexer = 'contextual'
            elif self.options.parser == 'earley':
                if self.options.postlex is not None:
                    logger.info("postlex can't be used with the dynamic lexer, so we use standard instead. "
                                "Consider using lalr with contextual instead of earley")
                    self.options.lexer = 'standard'
                else:
                    self.options.lexer = 'dynamic'
            elif self.options.parser == 'cyk':
                self.options.lexer = 'standard'
            else:
                assert False, self.options.parser
        lexer = self.options.lexer
        if isinstance(lexer, type):
            assert issubclass(lexer, Lexer)     ##

        else:
            assert_config(lexer, ('standard', 'contextual', 'dynamic', 'dynamic_complete'))
            if self.options.postlex is not None and 'dynamic' in lexer:
                raise ConfigurationError("Can't use postlex with a dynamic lexer. Use standard or contextual instead")

        if self.options.ambiguity == 'auto':
            if self.options.parser == 'earley':
                self.options.ambiguity = 'resolve'
        else:
            assert_config(self.options.parser, ('earley', 'cyk'), "%r doesn't support disambiguation. Use one of these parsers instead: %s")

        if self.options.priority == 'auto':
            self.options.priority = 'normal'

        if self.options.priority not in _VALID_PRIORITY_OPTIONS:
            raise ConfigurationError("invalid priority option: %r. Must be one of %r" % (self.options.priority, _VALID_PRIORITY_OPTIONS))
        assert self.options.ambiguity not in ('resolve__antiscore_sum', ), 'resolve__antiscore_sum has been replaced with the option priority="invert"'
        if self.options.ambiguity not in _VALID_AMBIGUITY_OPTIONS:
            raise ConfigurationError("invalid ambiguity option: %r. Must be one of %r" % (self.options.ambiguity, _VALID_AMBIGUITY_OPTIONS))

        if self.options.postlex is not None:
            terminals_to_keep = set(self.options.postlex.always_accept)
        else:
            terminals_to_keep = set()

        ##

        self.terminals, self.rules, self.ignore_tokens = self.grammar.compile(self.options.start, terminals_to_keep)

        if self.options.edit_terminals:
            for t in self.terminals:
                self.options.edit_terminals(t)

        self._terminals_dict = {t.name: t for t in self.terminals}

        ##

        ##

        if self.options.priority == 'invert':
            for rule in self.rules:
                if rule.options.priority is not None:
                    rule.options.priority = -rule.options.priority
        ##

        ##

        ##

        elif self.options.priority is None:
            for rule in self.rules:
                if rule.options.priority is not None:
                    rule.options.priority = None

        ##

        self.lexer_conf = LexerConf(
                self.terminals, re_module, self.ignore_tokens, self.options.postlex,
                self.options.lexer_callbacks, self.options.g_regex_flags, use_bytes=self.options.use_bytes
            )

        if self.options.parser:
            self.parser = self._build_parser()
        elif lexer:
            self.lexer = self._build_lexer()

        if cache_fn:
            logger.debug('Saving grammar to cache: %s', cache_fn)
            with FS.open(cache_fn, 'wb') as f:
                f.write(cache_md5.encode('utf8') + b'\n')
                pickle.dump(used_files, f)
                self.save(f)

    if __doc__:
        __doc__ += "\n\n" + LarkOptions.OPTIONS_DOC

    __serialize_fields__ = 'parser', 'rules', 'options'

    def _build_lexer(self, dont_ignore=False):
        lexer_conf = self.lexer_conf
        if dont_ignore:
            from copy import copy
            lexer_conf = copy(lexer_conf)
            lexer_conf.ignore = ()
        return TraditionalLexer(lexer_conf)

    def _prepare_callbacks(self):
        self._callbacks = {}
        ##

        if self.options.ambiguity != 'forest':
            self._parse_tree_builder = ParseTreeBuilder(
                    self.rules,
                    self.options.tree_class or Tree,
                    self.options.propagate_positions,
                    self.options.parser != 'lalr' and self.options.ambiguity == 'explicit',
                    self.options.maybe_placeholders
                )
            self._callbacks = self._parse_tree_builder.create_callback(self.options.transformer)
        self._callbacks.update(_get_lexer_callbacks(self.options.transformer, self.terminals))

    def _build_parser(self):
        self._prepare_callbacks()
        parser_class = get_frontend(self.options.parser, self.options.lexer)
        parser_conf = ParserConf(self.rules, self._callbacks, self.options.start)
        return parser_class(self.lexer_conf, parser_conf, options=self.options)

    def save(self, f):
        #--
        data, m = self.memo_serialize([TerminalDef, Rule])
        pickle.dump({'data': data, 'memo': m}, f, protocol=pickle.HIGHEST_PROTOCOL)

    @classmethod
    def load(cls, f):
        #--
        inst = cls.__new__(cls)
        return inst._load(f)

    def _deserialize_lexer_conf(self, data, memo, options):
        lexer_conf = LexerConf.deserialize(data['lexer_conf'], memo)
        lexer_conf.callbacks = options.lexer_callbacks or {}
        lexer_conf.re_module = regex if options.regex else re
        lexer_conf.use_bytes = options.use_bytes
        lexer_conf.g_regex_flags = options.g_regex_flags
        lexer_conf.skip_validation = True
        lexer_conf.postlex = options.postlex
        return lexer_conf

    def _load(self, f, **kwargs):
        if isinstance(f, dict):
            d = f
        else:
            d = pickle.load(f)
        memo = d['memo']
        data = d['data']

        assert memo
        memo = SerializeMemoizer.deserialize(memo, {'Rule': Rule, 'TerminalDef': TerminalDef}, {})
        options = dict(data['options'])
        if (set(kwargs) - _LOAD_ALLOWED_OPTIONS) & set(LarkOptions._defaults):
            raise ConfigurationError("Some options are not allowed when loading a Parser: {}"
                             .format(set(kwargs) - _LOAD_ALLOWED_OPTIONS))
        options.update(kwargs)
        self.options = LarkOptions.deserialize(options, memo)
        self.rules = [Rule.deserialize(r, memo) for r in data['rules']]
        self.source_path = '<deserialized>'
        parser_class = get_frontend(self.options.parser, self.options.lexer)
        self.lexer_conf = self._deserialize_lexer_conf(data['parser'], memo, self.options)
        self.terminals = self.lexer_conf.terminals
        self._prepare_callbacks()
        self._terminals_dict = {t.name: t for t in self.terminals}
        self.parser = parser_class.deserialize(
            data['parser'],
            memo,
            self.lexer_conf,
            self._callbacks,
            self.options,  ##

        )
        return self

    @classmethod
    def _load_from_dict(cls, data, memo, **kwargs):
        inst = cls.__new__(cls)
        return inst._load({'data': data, 'memo': memo}, **kwargs)

    @classmethod
    def open(cls, grammar_filename, rel_to=None, **options):
        #--
        if rel_to:
            basepath = os.path.dirname(rel_to)
            grammar_filename = os.path.join(basepath, grammar_filename)
        with open(grammar_filename, encoding='utf8') as f:
            return cls(f, **options)

    @classmethod
    def open_from_package(cls, package, grammar_path, search_paths=("",), **options):
        #--
        package = FromPackageLoader(package, search_paths)
        full_path, text = package(None, grammar_path)
        options.setdefault('source_path', full_path)
        options.setdefault('import_paths', [])
        options['import_paths'].append(package)
        return cls(text, **options)

    def __repr__(self):
        return 'Lark(open(%r), parser=%r, lexer=%r, ...)' % (self.source_path, self.options.parser, self.options.lexer)


    def lex(self, text, dont_ignore=False):
        #--
        if not hasattr(self, 'lexer') or dont_ignore:
            lexer = self._build_lexer(dont_ignore)
        else:
            lexer = self.lexer
        lexer_thread = LexerThread(lexer, text)
        stream = lexer_thread.lex(None)
        if self.options.postlex:
            return self.options.postlex.process(stream)
        return stream

    def get_terminal(self, name):
        #--
        return self._terminals_dict[name]
    
    def parse_interactive(self, text=None, start=None):
        return self.parser.parse_interactive(text, start=start)

    def parse(self, text, start=None, on_error=None):
        #--
        return self.parser.parse(text, start=start, on_error=on_error)

    @property
    def source(self):
        warn("Lark.source attribute has been renamed to Lark.source_path", DeprecationWarning)
        return self.source_path

    @source.setter
    def source(self, value):
        self.source_path = value

    @property
    def grammar_source(self):
        warn("Lark.grammar_source attribute has been renamed to Lark.source_grammar", DeprecationWarning)
        return self.source_grammar

    @grammar_source.setter
    def grammar_source(self, value):
        self.source_grammar = value



class DedentError(LarkError):
    pass

class Indenter(PostLex):
    def __init__(self):
        self.paren_level = None
        self.indent_level = None
        assert self.tab_len > 0

    def handle_NL(self, token):
        if self.paren_level > 0:
            return

        yield token

        indent_str = token.rsplit('\n', 1)[1] ##

        indent = indent_str.count(' ') + indent_str.count('\t') * self.tab_len

        if indent > self.indent_level[-1]:
            self.indent_level.append(indent)
            yield Token.new_borrow_pos(self.INDENT_type, indent_str, token)
        else:
            while indent < self.indent_level[-1]:
                self.indent_level.pop()
                yield Token.new_borrow_pos(self.DEDENT_type, indent_str, token)

            if indent != self.indent_level[-1]:
                raise DedentError('Unexpected dedent to column %s. Expected dedent to %s' % (indent, self.indent_level[-1]))

    def _process(self, stream):
        for token in stream:
            if token.type == self.NL_type:
                for t in self.handle_NL(token):
                    yield t
            else:
                yield token

            if token.type in self.OPEN_PAREN_types:
                self.paren_level += 1
            elif token.type in self.CLOSE_PAREN_types:
                self.paren_level -= 1
                assert self.paren_level >= 0

        while len(self.indent_level) > 1:
            self.indent_level.pop()
            yield Token(self.DEDENT_type, '')

        assert self.indent_level == [0], self.indent_level

    def process(self, stream):
        self.paren_level = 0
        self.indent_level = [0]
        return self._process(stream)

    ##

    @property
    def always_accept(self):
        return (self.NL_type,)


import pickle, zlib, base64
DATA = (
b'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'
)
DATA = pickle.loads(zlib.decompress(base64.b64decode(DATA)))
MEMO = (
b'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'
)
MEMO = pickle.loads(zlib.decompress(base64.b64decode(MEMO)))
Shift = 0
Reduce = 1
def Lark_StandAlone(**kwargs):
  return Lark._load_from_dict(DATA, MEMO, **kwargs)
